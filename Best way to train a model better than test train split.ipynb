{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8ccba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042f4c8d",
   "metadata": {},
   "source": [
    "# points to be covered in this are \n",
    "## drawback for train test split procedure for model evalution\n",
    "## K fold cross validation overcome the drawback of it \n",
    "## how can cross validation be used for tunning parameters choosing between the models and the feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2160e0",
   "metadata": {},
   "source": [
    "# NEed a eay to choose between ml models \n",
    "## and main goal is to estimate likely performance on the out of sample data\n",
    "# Initial idea : Train test the same model \n",
    "## but maximizing training accuracy rewards in over complex models which do not generalises and over fit the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed93de08",
   "metadata": {},
   "source": [
    "# alterantive idea : Train Test split \n",
    "## split dataset into two spices one training the model and other for testing the model \n",
    "## testing accuracy is a better estimate then training accuracy of out of sample performance\n",
    "## But it provides a high variance since changing which observation happens to be in testing set can significantly change the testing accuracy meaning the testing accuracy can change a lot depending upon the what values are there in the testing matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18d127bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c4b5958",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "X = iris.data # this are the features \n",
    "Y = iris.target  # these are responses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5826b77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "X_train , X_test ,Y_train , Y_test = train_test_split(X,Y,random_state=4)# as the random state changes \n",
    "                                                                         # the output also changes out well in 2 it gives 100%\n",
    "# this is why the testing accuracy is a high variance estimate\n",
    "# check the classification accuracy of knn with k=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train,Y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(metrics.accuracy_score(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f469ae",
   "metadata": {},
   "source": [
    "# what if we create a bunch of train_test_split calculate the testing accuracy for each and averaged the result together\n",
    "\n",
    "# this is the essence of cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc2e9c",
   "metadata": {},
   "source": [
    "# steps for k-fold cross validation\n",
    "## 1. split the dataset into k equal partitions (or folds)\n",
    "## 2. use fold one as the testing set and the union of the other folds as the training set \n",
    "## 3.Calculate the testing accuracy \n",
    "## 4. repeat 2 and 3, K times using a different fold asthe testing set each time \n",
    "## 5. USe the average testing accuracy as the estimate of out of sample accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f3083e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24]\n"
     ]
    }
   ],
   "source": [
    "l=[]\n",
    "for i in range(25):\n",
    "    l.append(i)\n",
    "X_used_for_demonstration = np.array(l)\n",
    "print(X_used_for_demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2bf03dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of folds there are 5\n",
      "\n",
      "\n",
      "                  Training set observations                   Testing set observations\n",
      "[ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]  [0 1 2 3 4]\n",
      "[ 0  1  2  3  4 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]  [5 6 7 8 9]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 15 16 17 18 19 20 21 22 23 24]  [10 11 12 13 14]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 20 21 22 23 24]  [15 16 17 18 19]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]  [20 21 22 23 24]\n"
     ]
    }
   ],
   "source": [
    "# Exaaample how to split a dataset \n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5,shuffle=False)   # True karoge toh no. shuffle hoga\n",
    "kf_used = kf.get_n_splits(25) # this is to just see how to split in new sklearn\n",
    "print(\"no. of folds there are \"+str(kf_used))\n",
    "print(\"\\n\")\n",
    "\n",
    "print('{:^61} {}'.format('Training set observations','Testing set observations'))\n",
    "for sett, data in kf.split(X_used_for_demonstration):\n",
    "    print(\"{}  {}\".format(sett,data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42827fdc",
   "metadata": {},
   "source": [
    "# 1. dataset 25 observations numbered from 0 to 24\n",
    "# 2. 5 fold cross validation thus it runs for 5 iterations\n",
    "# 3. for each iteration every observation in either of the dataset is are different \n",
    "# 4. every observstion in testing set is exactly once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82f36687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benifits of K fold cross validations\n",
    "## more accurate estimate of out of sample accuracy\n",
    "## more efficient use of data as every observation is used as both in training set and the testing set \n",
    "\n",
    "# Benifits of train test split method \n",
    "## it is K time faster \n",
    "## simpler to examione the detailed result of the testing process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984854e6",
   "metadata": {},
   "source": [
    "# Cross validation Recommendations\n",
    "## 1. K can be any no. , but k =10 is generally reccomended \n",
    "## 2. for classification problems , statified sampling is recommended for creating the folds\n",
    "### a. each response class should be represented with equal proportion in each of the k folds which means if we observe that there are 20% of ham mails in ham and spam classificastion then each set should have 20% of ham mails in them \n",
    "### b. sklearn cross_val_score function does this by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17369ed9",
   "metadata": {},
   "source": [
    "#  Cross Validation example parameter tuning \n",
    "## select the best tunning parameter (aka hyperparameter) fot KNN on the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc492aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5deebef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.93333333 1.         1.         0.86666667 0.93333333\n",
      " 0.93333333 1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "# 10 folds cross valdation with k=5 for knn (the neighbour parameter)\n",
    "knn_for_cross_val = KNeighborsClassifier(n_neighbors=5)            # X = Feature matrix, Y = response value\n",
    "scores = cross_val_score(knn_for_cross_val,X,Y,cv=10,scoring='accuracy') \n",
    "# Args(model_object,X,Y,cv(kitna folds pe),scoring(evaluation matrix))\n",
    "# here we do not take care of spliting the data cross val score takes care of that \n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9465906e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "print(scores.mean()) # this the fifth method of cross evalv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6403b8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668, 0.9666666666666668]\n"
     ]
    }
   ],
   "source": [
    "# serach for the optimum value of k in knn\n",
    "k_range = range(1,31)\n",
    "k_scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn_for_cross_val,X,Y,cv=10,scoring = \"accuracy\")\n",
    "    k_scores.append(scores.mean())\n",
    "print(k_scores) # this would not vary here as it as inbuilt dataset and for very long years thus it gives\n",
    "# similar output  each time as it ahs been trained on it for so many times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d916c",
   "metadata": {},
   "source": [
    "# for knn the high Value of K gives lower complexcity model thus gives the simplest one which we require"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a470fe6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cross Valuated Accuracy')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plot the value of k for knn (x axis) vs value for cross validation accuracy (y axis)\n",
    "plt.plot(k_range,k_scores)\n",
    "plt.xlabel(\"Value of K for Knn\")\n",
    "plt.ylabel(\"Cross Valuated Accuracy\")\n",
    "# in this if it has a inverted u shaped curve which we will generally observe \n",
    "# low values are less biased but high variance \n",
    "# high values are more biased but low variance \n",
    "# thus the best value is the one in the middle as it equallises both biasness and variance \n",
    "# thus gives a generalised output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cac2ec",
   "metadata": {},
   "source": [
    "# Cross Validation Example Model selection \n",
    "## compare the best knn model with logistic regression on the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94dc272a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "print(cross_val_score(knn_for_cross_val,X,Y,cv=10,scoring = \"accuracy\").mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f82128ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9733333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mynam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mynam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mynam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mynam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mynam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# 10 fold cross validation with the logistic regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "print((cross_val_score(logreg,X,Y,cv=10,scoring='accuracy')).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078c1a69",
   "metadata": {},
   "source": [
    "# mean classification accuracy is a reward function which we wanna maximize but MSE is loss function means which we wanna minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b86b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cross_val_score on gareth dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25c4cfb",
   "metadata": {},
   "source": [
    "# Improvement of Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a07dbd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
